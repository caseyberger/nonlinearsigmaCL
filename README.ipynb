{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdcfde35-a175-460c-bb77-923580bafd08",
   "metadata": {},
   "source": [
    "# Simulation and Analysis Code for $2D$ $\\mathcal{O}(3)$ Nonlinear Sigma Model with Topological Term $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7d3f9f-040a-4a03-80f1-6cefd1af5660",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Simulation code for Monte Carlo approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee512d33-d8b1-4cf5-a788-0915b300712d",
   "metadata": {},
   "source": [
    "This approach uses a conventional Monte Carlo simulation with a Metropolis step to simulate the $2D$ $\\mathcal{O}(3)$ nonlinear sigma model where the topological term $\\theta$ is assigned an imaginary value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a521133-d8c4-4e42-900a-f9ab913f9aa2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Background and Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb7b1e3-fc77-4db1-9fe7-5a928507c0e7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Lattice Action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7fb86b-c516-4a77-bf06-da8cb396e0cc",
   "metadata": {},
   "source": [
    "In the continuum, the model has action\n",
    "\n",
    "$S = \\frac{1}{2g} \\int d^{2} x \\left( \\partial_{\\mu} \\vec{\\phi}(x)\\right)^{2} - i \\theta \\int d^{2} x Q(x)$\n",
    "\n",
    "with\n",
    "\n",
    "$Q(x) = \\frac{1}{8 \\pi} \\epsilon^{\\mu \\nu} \\epsilon_{abc} \\partial_{\\mu} \\phi^{b}(x) \\partial_{\\nu}\\phi^{c}(x)$\n",
    "\n",
    "with $\\phi$ a 3-component unit vector.\n",
    "\n",
    "In lattice action form, we have (see B. Alles, M. Giordano, and A. Papa. Behavior near θ = π of the mass gap in the two-dimensional o(3) nonlinear sigma model. Phys. Rev. B, 90:184421, Nov 2014):\n",
    "\n",
    "$\n",
    "\\begin{eqnarray}\n",
    "S_{L} &=& A_{L} - i \\theta Q_{L}\\\\\n",
    "A_{L} &=& -\\frac{1}{g_{L}}\\sum_{x,\\mu}\\left(\\phi_{x}\\cdot \\phi_{x+\\hat{\\mu}}\\right)\\\\\n",
    "Q_{L} &=& \\sum_{x}\\sum_{\\Delta} Q_{L} \\Delta\n",
    "\\end{eqnarray}\n",
    "$\n",
    "\n",
    "where $\\vec{\\phi}$ a 3-component unit vector ($\\vec{\\phi} \\cdot\\vec{\\phi} = 1$) and $Q_{L}$ is the total topological charge on the lattice. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91aa7853-0471-4a99-9be9-e5f2113fc26a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Regularization of the topological charge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0e2225-bb1d-4e43-a503-74301ff4513f",
   "metadata": {},
   "source": [
    "The topological charge has been defined via sums over triangles created by cutting each square plaquette along the diagonal. Each vertex is labeled (numbered counter-clockwise), such that we call the fields at the sites of the vertices $\\vec{\\phi}_{1}$, $\\vec{\\phi}_{2}$, and $\\vec{\\phi}_{3}$. \n",
    "\n",
    "\n",
    "The topological charge over each triangle obeys\n",
    "\n",
    "$\\exp(2 \\pi i Q_{L}(\\Delta)) = \\frac{1}{\\rho}\\left(1 + \\vec{\\phi}_{1}\\cdot\\vec{\\phi}_{2} + \\vec{\\phi}_{2}\\cdot\\vec{\\phi}_{3} + \\vec{\\phi}_{3}\\cdot\\vec{\\phi}_{1} + i \\vec{\\phi}_{1} \\cdot (\\vec{\\phi}_{2}\\times\\vec{\\phi}_{3})\\right)$\n",
    "\n",
    "with \n",
    "\n",
    "$\\rho^{2} = 2(1+\\vec{\\phi}_{1}\\cdot\\vec{\\phi}_{2})(1 + \\vec{\\phi}_{2}\\cdot\\vec{\\phi}_{3})(1+ \\vec{\\phi}_{3}\\cdot\\vec{\\phi}_{1})$ \n",
    "\n",
    "and \n",
    "\n",
    "$Q_{L}(\\Delta) \\in \\left[-\\frac{1}{2}, \\frac{1}{2}\\right]$\n",
    "\n",
    "We use the arcsin of the quantity $\\exp(2 \\pi i Q_{L}(\\Delta))$ to compute $Q_{L}(\\Delta)$, as in C++ the domain of arcsin is symmetric about $0$, which prevents the need to adjust the domain to fit the expectation given above.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "When we sum over all unique triangles on the lattice, the total topological charge $Q_{L} = \\sum_{\\Delta} Q_{}(\\Delta)$ should return integer values, however we are currently not finding this to be the case. More investigation is required here.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9664cae-8636-4090-8e41-5c86536d8df9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Analytic continuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f68185-4f87-4662-a170-3301619c8c72",
   "metadata": {},
   "source": [
    "The simulation is run for imaginary values of $\\theta$, which means we must analytically continue our results for real $\\theta$. To do this, we fit our results to a curve and then substitute our imaginary $i \\theta = \\nu$ for a real $\\theta = -i \\nu$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522cc3cf-0501-4d90-93a0-3fac19d6111f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### The mass gap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd73dcb-94b4-4845-bb96-c0b384d12c66",
   "metadata": {},
   "source": [
    "Our goal with this project is to determine the mass gap, which should vanish as $\\theta \\to \\pi$. The mass gap is the inverse of the correlation length, which we compute in the simulation.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "The correlation length should be a real number, but our simulations are currently returning a complex result, due to the complexity of the correlation function. More investigation is required here.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683f1fad-7431-4c70-bbfc-3df59823a4e2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Running the simulation code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06da23c0-3ca3-4224-93be-660676fbacb8",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Makefile flags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e133b9-bdb4-4e3c-bd80-25e7dd926550",
   "metadata": {},
   "source": [
    "The code is written in C++ and OpenMP and can be compiled with numerous flags.\n",
    "\n",
    "```bash\n",
    "USE_OMP ?= TRUE\n",
    "USE_GPROF ?= FALSE\n",
    "USE_TEST_PRINT_STATEMENTS ?= FALSE\n",
    "USE_EXTREME_TEST_CONDITION ?= FALSE\n",
    "USE_CHECK_QL_COS ?= FALSE\n",
    "USE_CONST_RN ?= FALSE\n",
    "```\n",
    "\n",
    "The first flag \"USE_OMP\" toggles whether to implement the parallelization in the code. It should be set to \"TRUE\" if you want to run the simulation in parallel. This is highly recommended for large lattices as the scaling is very poor in series.\n",
    "\n",
    "If you wish to profile the code, set the second flag \"USE_GPROF\" to \"TRUE\". This sets the correct compiler flags so that you can generate the profiling output. To view the output after the code has run, go to the directory in which you have the executable and run the command\n",
    "```bash\n",
    "gprof -l nonlinearsigma gmon.out > profiling_results.txt\n",
    "```\n",
    "You will then be able to see the profiling report. In general, you should set \"USE_GPROF\" to false, unless you are looking to optimize the code or troubleshoot it.\n",
    "\n",
    "The flag \"USE_TEST_PRINT_STATEMENTS\" activates print statements throughout the code. This is useful for debugging, but should generally be set to FALSE as it slows down the code.\n",
    "\n",
    "If you run into major problems, set \"USE_EXTREME_TEST_CONDITION\" to TRUE. This will run a testing suite built into the code, but will not run the usual simulation. This can help you identify problems in the code, and the testing suite is a function inside the main function, which can be modified as needed to add more tests.\n",
    "\n",
    "To switch from using arcsin to calculate $Q_{L}$ to using arccos, set \"USE_CHECK_QL_COS\" to TRUE. In general, this should be set to FALSE< as we want to use arcsin due to its useful symmetry.\n",
    "\n",
    "Finally, if you want to remove the random number generation and use a constant value for the random numbers, set \"USE_CONST_RN\" to true. This should only be done when testing the code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4a05c8-0567-4c06-a500-81504bf667f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Compiling the code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c4ad29-fa67-40a9-963e-17fc67e93b1c",
   "metadata": {},
   "source": [
    "Once you have set the flags you want to use, you can compile the code by typing\n",
    "\n",
    "```bash\n",
    "make -f make_sigma\n",
    "```\n",
    "It can be useful to run \n",
    "\n",
    "```bash\n",
    "make -f make_sigma clean\n",
    "```\n",
    "first to clear out old .o files that might not be updated otherwise.\n",
    "\n",
    "Once you have compiled the code, you should have an executable by the name of ```nonlinearsigma``` which you will use to run the simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65455eb3-f0ec-4c15-9daa-386088ff5776",
   "metadata": {},
   "source": [
    "#### Other important things to note before running the code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62289d5-34c2-49d3-bad7-b7f18c0bae56",
   "metadata": {},
   "source": [
    "The simulation requires a list of parameters. We give those to the code through the text file ```inputs.txt```. That file consists of a list of parameter keywords and a value. Be very careful not to change the format of this document, only the numbers, otherwise the simulation won't be able to understand what values get assigned to what parameters. \n",
    "\n",
    "The file ```inputs.txt``` should look like this\n",
    "\n",
    "```\n",
    "L = 10\n",
    "beta = 1.6\n",
    "itheta = 1.\n",
    "ntherm = 1000\n",
    "nMC = 1000\n",
    "freq = 100\n",
    "```\n",
    "Where ```L``` gives the length of the square lattice, ```beta``` is $\\beta = 1/g_{L}$ and should be set to 1.6, ```itheta``` is the imaginary value given for the topological term and is given in fractions of $\\pi$ (so you should enter 0.5 if you want $i \\theta = \\pi/2$), ```ntherm``` is the number of steps you want the simulation to take for thermalization, ```nMC``` is the number of steps in the Monte Carlo loop after thermalization, and ```freq``` sets the number of steps between saved configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb96d86b-7b56-44b7-950b-661184b8e87a",
   "metadata": {},
   "source": [
    "#### Running the code -- single job in interactive node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019052a4-051e-4d54-8c6e-ceb9f000019e",
   "metadata": {},
   "source": [
    "If you want to run this in an interactive node to test, you can request an interactive node with the following command:\n",
    "\n",
    "```bash\n",
    "salloc --cpus-per-task=1 --time=00:30:00\n",
    "```\n",
    "where you can adjust the number of cpus per task if you want to run the parallelized code and the time requested is in format hh:mm:ss\n",
    "\n",
    "You can run the job using the command\n",
    "\n",
    "```bash\n",
    "./nonlinearsigma inputs.txt\n",
    "```\n",
    "```nonlinearsigma``` is the name of the executable created when you compiled the code, and inputs.txt is the list of inputs mentioned above. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afeaa6d-f429-4abf-8907-8c332d05b4e2",
   "metadata": {},
   "source": [
    "#### Running the code -- single SLURM submission "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6355d437-a547-410c-ae57-cbee6abd16c4",
   "metadata": {},
   "source": [
    "Once you're comfortable with the code and want to submit a job that's longer than your interactive session (or one that may need to run overnight, etc), you do that by writing a SLURM script and sending that script to the scheduler.\n",
    "\n",
    "The script is called ```submit_sigma.sh``` and looks like this:\n",
    "```bash\n",
    "#SBATCH --job-name=nonlinearsigma_omp_test           # Job name\n",
    "#SBATCH --mail-type=ALL                              # Mail events (NONE, BEGIN, END, FAIL, ALL)\n",
    "#SBATCH --mail-user=cberger@smith.edu                # Where to send mail\n",
    "#SBATCH --partition=phyq                             # Which partition to use\n",
    "#SBATCH --nodes=1                                    # Number of nodes\n",
    "#SBATCH --cpus-per-task=1                           # Number of threads per task (OpenMP)\n",
    "#SBATCH --mem=1gb                                    # Job memory request\n",
    "##SBATCH --time=05:00:00                             # Time limit hrs:min:sec\n",
    "#SBATCH --output=nonlinearsigma_omp_test_%j.log      # Standard output \n",
    "#SBATCH --error=err_nonlinearsigma_omp_test_%j.log   # Standard output and error log\n",
    "\n",
    "pwd; hostname; date\n",
    "\n",
    "export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n",
    "\n",
    "echo \"Running nonlinear sigma on single CPU core\"\n",
    "\n",
    "/usr/bin/time -v ./nonlinearsigma inputs.txt\n",
    "\n",
    "date\n",
    "```\n",
    "\n",
    "Let's walk through what this script does.\n",
    "\n",
    "The first line:\n",
    "```bash\n",
    "#SBATCH --job-name=nonlinearsigma_omp_test           # Job name\n",
    "```\n",
    "Assigns a name to the job, which can help you keep track of what is running in the queue. I tend to name all my jobs in a certain phase of the work the same thing (e.g. \"nonlinearsigma_small_L_tests\" if I'm testing out the script on small lattices or \"nonlinearsigma_first_production_run\" if I'm starting to take data for real). This is just for your own information, so name it whatever you'd like.\n",
    "\n",
    "The next two lines\n",
    "```bash\n",
    "#SBATCH --mail-type=ALL                              # Mail events (NONE, BEGIN, END, FAIL, ALL)\n",
    "#SBATCH --mail-user=cberger@smith.edu                # Where to send mail\n",
    "```\n",
    "tell SLURM who to email and when to email you. I have it set to email me at my Smith email anytime a job starts, ends, or fails. I find this helpful, especially if trouble arises, but it's your choice what to put here (just don't leave my email address in!).\n",
    "\n",
    "The next line\n",
    "```bash\n",
    "#SBATCH --partition=phyq                             # Which partition to use\n",
    "```\n",
    "sends the jobs to the partition that belongs to the physics department. We have priority on this node, but we can request time on other nodes if we really need to. We'd need to talk to CATS about that if we wanted to do it.\n",
    "\n",
    "The next lines specify the code's needs:\n",
    "```bash\n",
    "#SBATCH --nodes=1                                    # Number of nodes\n",
    "#SBATCH --cpus-per-task=1                           # Number of threads per task (OpenMP)\n",
    "#SBATCH --mem=1gb                                    # Job memory request\n",
    "```\n",
    "Since we are using OpenMP for parallelization, we only need one node, but we will want to change ```bash --cpus-per-task``` to something larger for OpenMP. Unless doing a very large lattice (L > 100), I tend to ask for 30 CPUs per task, which allows for 2 jobs to run at a time on each node. If you're doing something large, you might want to consider asking for 60 CPUs per task, which will occupy an entire node.\n",
    "\n",
    "This line\n",
    "```bash\n",
    "##SBATCH --time=05:00:00                             # Time limit hrs:min:sec\n",
    "```\n",
    "sets a time limit -- it will cut off your code when that limit is reached, whether it is done or not. Note there is an extra `#` here -- that means it's commented out, so it will not have a time limit. I tend to only use the time limits when testing.\n",
    "\n",
    "These next two lines\n",
    "```bash\n",
    "#SBATCH --output=nonlinearsigma_omp_test_%j.log      # Standard output \n",
    "#SBATCH --error=err_nonlinearsigma_omp_test_%j.log   # Standard output and error log\n",
    "``` \n",
    "give names to the output and error files. These files are created when the code runs and can help you debug if things go wrong.\n",
    "\n",
    "All the above were instructions for SLURM, which schedules jobs on the machine's available resources. Now we get into the commands to actually run the code.\n",
    "\n",
    "```bash\n",
    "pwd; hostname; date\n",
    "```\n",
    "This just prints where the job is being run from and the date and time before the job starts.\n",
    "\n",
    "```bash\n",
    "export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n",
    "```\n",
    "this sets the number of threads in OpenMP equal to the number of CPUs per task you assigned above.\n",
    "\n",
    "```bash\n",
    "echo \"Running nonlinear sigma\"\n",
    "\n",
    "/usr/bin/time -v ./nonlinearsigma inputs.txt\n",
    "```\n",
    "This prints out an annoucement that you're starting the job, which is useful in the logfile, and then runs the simulation with the inputs file. Make sure you have the inputs file and the executable in the same folder as the SLURM script so the computer can find them.\n",
    "\n",
    "And finally\n",
    "```bash\n",
    "date\n",
    "```\n",
    "we print the datetime stamp at the end of the simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2caf7f5-a9fd-4fa8-af14-1f122340fed9",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Running the code -- batch SLURM submissions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242ccdae-80ba-49e1-8a2f-daa7d4386c45",
   "metadata": {},
   "source": [
    "Each inputs file is one set of parameters, and we need to get lots of data. If you're not interested in manually setting up these SLURM scripts, input files, etc by hand, I don't blame you. That's why I wrote a Python code to do most of the work for you.\n",
    "\n",
    "In this code, you specify what parameters you want to run, and the script creates all the appropriate directories and puts the correct inputs file, slurm script, and the executable in each directory. You still have to go in and submit the files yourself, but it's much easier than writing all these scripts yourself.\n",
    "\n",
    "The script is called ```create_input_files.py``` and here is the part you will need to modify:\n",
    "\n",
    "```python\n",
    "#beta = 1/g = 1.6\n",
    "beta = 1.6\n",
    "#number of steps in thermalization\n",
    "ntherm = 4000\n",
    "#number of monte carlo steps\n",
    "nMC = 10000\n",
    "#number of steps between samples\n",
    "freq = 100\n",
    "#list of values for lattice length L\n",
    "L_list = [10,40,80,120,180]\n",
    "#list of values for itheta (as fractions of pi)\n",
    "itheta_list = [0.0,0.125,0.25,0.375,0.5,0.625,0.75,0.875,1.,1.125]\n",
    "\n",
    "script_name = \"nonlinearsigma\"\n",
    "job_name = \"nlsigma_prelim_tests\"\n",
    "email = \"cberger@smith.edu\"\n",
    "num_cpus = 30\n",
    "\n",
    "```\n",
    "Beta, ntherm, nMC, and freq all take one number as input, but you can create a list of the number of lattice lengths you want, and the values for itheta (remember these are fractions of pi). \n",
    "\n",
    "```script_name``` is the executable, so it should be \"nonlinearsigma\", but ```job_name``` is what will be put in for the job name in the SLURM script. Similarly, you can enter the email address you want included in the SLURM script and choose how many CPUs you want.\n",
    "\n",
    "Once you've modified this script to have the values you want, put it in a directory for this batch. I tend to name those something like ```run_yyymmdd```. Also in that directory should be the executable, so copy that in once you've compiled the code.\n",
    "\n",
    "Then, inside the batch directory, go ahead and run the python script\n",
    "\n",
    "```bash\n",
    "python create_input_files.py\n",
    "```\n",
    "\n",
    "When it's done, you should see the subdirectories created -- one for each job. You need to go into each subdirectory to submit the jobs using\n",
    "\n",
    "```bash\n",
    "sbatch submit_sigma.sh\n",
    "```\n",
    "and it will submit the jobs to the scheduler.\n",
    "\n",
    "You can run this with \n",
    "```bash\n",
    "sbatch submit_sigma.sh\n",
    "```\n",
    "and you can check the status of your jobs any time with the command\n",
    "```bash\n",
    "squeue -u your_username\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713bc050-a832-481b-9ae1-583a53d02fbc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Python Code for Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b66bc9-39fa-4947-b729-a9402019265d",
   "metadata": {},
   "source": [
    "There are a number of Jupyter Notebooks prepared to explore the data that comes out of the simulation. These are all located in the Analysis folder. There is also a Python class, LatticeData.py, which creates an analyzer object that has internal functions for analysis and visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce14a98-0091-41a6-8760-62b79073d257",
   "metadata": {
    "tags": []
   },
   "source": [
    "### LatticeData.py - Data Analyzer Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f77ded4-03b6-4148-8945-ae5c5ce2e745",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Importing and initializing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954184d7-b929-48cf-8e8d-3f1a083e5f6b",
   "metadata": {},
   "source": [
    "This is a Python class. You can create a lattice data object, which then holds all the functionality you need to analyze and visualize the data. You will need to import this into your notebook like this:\n",
    "```python\n",
    "from LatticeData import *\n",
    "```\n",
    "\n",
    "You will want to also import the following:\n",
    "* Numpy\n",
    "* Pandas\n",
    "* Matplotlib\n",
    "* Seaborn\n",
    "\n",
    "The simplest way to start now is by initializing the object:\n",
    "\n",
    "``` python\n",
    "analyzer = LatticeData()\n",
    "```\n",
    "You can now do a number of operations with this analyzer. The full list of functions is included later, but if you wanted to get all the data from that folder and put it into a Pandas dataframe, that would look like this:\n",
    "\n",
    "```python\n",
    "df = analyzer.get_data()\n",
    "```\n",
    "\n",
    "There are a number of default settings in this class, which you can change when you initialize. The defaults are:\n",
    "```python\n",
    "analyzer_default = LatticeData(datadir = \"/data/\", header = \"nonlinearsigma_data\",\n",
    "                 dirheader = \"nlsigma_data\", Gheader = \"Gij_avg_nonlinearsigma_data\", \n",
    "                 tol = 0.00001, palette = \"viridis\")\n",
    "```\n",
    "which initializes the following internal variables:\n",
    "```python\n",
    "\n",
    "self.path = os.getcwd()+datadir #location of data\n",
    "self.header = header #set the start of the filename for the data files\n",
    "self.dirheader = dirheader #set the start of the data directory name from the runs\n",
    "self.Gheader = Gheader #set the start of the filename for correlation function files\n",
    "self.tol = tol #set the error range for parameters -- this is for filtering\n",
    "self.palette = palette #option to change seaborn palette\n",
    "self.observables = ['Q_L', 'A_L', 'S_L', 'Xi_L'] #observables whose expectation values can be computed\n",
    "self.parameters = [\"itheta\", \"beta\", \"length\",\"nMC\", \"ntherm\", \"freq\"] #parameters read in by the simulation code\n",
    "```\n",
    "Most of these will not need to be changed, but let's say you want to analyze a special batch of data, which you've stored in a directory called ```data_test```, and you want to use a different visualization palette, you could intialize the object like this:\n",
    "```python\n",
    "analyzer_special = LatticeData(datadir = \"/data_test/\", palette = \"magma\")\n",
    "```\n",
    "(You can choose any [seaborn palette](https://seaborn.pydata.org/tutorial/color_palettes.html) for this)\n",
    "\n",
    "Then when you run the function\n",
    "```python  \n",
    "df = analyzer_special.get_data()\n",
    "```\n",
    "it will aggregate all the data files in the folder ```data_test```.\n",
    "\n",
    "Below is a complete list of functions for the LatticeData class, with a brief description. While Python doesn't have the same public/private distinctions as C++, I've organized them into those same groups. Public functions are things that you may want to use. Private functions are functions that you should never need to call yourself, but are called internally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244be6b2-b1b3-4584-969d-caf16f7a08f8",
   "metadata": {},
   "source": [
    "#### Lattice Data Class Built-In Functions (\"Public\" or external)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b064276-523d-470d-b85a-823634926d25",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### copy_data_from_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09c0882-d614-4d54-8bd9-08d267ec3e03",
   "metadata": {},
   "source": [
    "```python\n",
    "copy_data_from_directory(self, src_dir, dst_path = None)\n",
    "```\n",
    "This function loops through a specified, directory, finds any simulation directories (directions that begin with \"nlsigma_data\" or whatever you have specified under ```dirheader``` in your intialization), and copies the .csv files within those directories to some destination directory. The default destination directory is whatever you've specified for your data directory.\n",
    "    The function requires a source directory to be passed as a string -- this is the directory where you have all your simulation results that you want copied over -- and gives you the option to specify a different destination directory using ```dst_path```)\n",
    "    \n",
    "If some of your simulations are not complete yet (determined by testing whether the .csv has the correct number of lines), this function will not copy those files and will print out the name of the run and how many lines there are in the data output file. For example, the folder ```run_7_18_23_stats``` contains some runs that haven't finished yet. If I try to copy the data from that directory into my data directory:\n",
    "    \n",
    "```python\n",
    "analyzer.copy_data_from_directory(\"run_7_18_23_stats\")\n",
    "```\n",
    "here's what appears printed out:\n",
    "```\n",
    "run L_180_beta_1.600000_itheta_0.000000_ntherm_5000_nMC_50000_freq_100 not yet complete: 378 lines\n",
    "run L_180_beta_1.600000_itheta_2.356194_ntherm_5000_nMC_50000_freq_100 not yet complete: 391 lines\n",
    "run L_180_beta_1.600000_itheta_3.141593_ntherm_5000_nMC_50000_freq_100 not yet complete: 237 lines\n",
    "run L_180_beta_1.600000_itheta_1.570796_ntherm_5000_nMC_50000_freq_100 not yet complete: 381 lines\n",
    "run L_180_beta_1.600000_itheta_0.785398_ntherm_5000_nMC_50000_freq_100 not yet complete: 240 lines\n",
    "```\n",
    "And those runs will not be in the data directory, while completed runs will have been copied in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8be54de-5f43-4abc-b25e-3296d52afdb6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### all_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38426e5a-a4f7-4c89-a54e-5e5e520df3ee",
   "metadata": {},
   "source": [
    "```python\n",
    "all_params(self)\n",
    "```\n",
    "This function collects every unique set of parameters from your default data directory and returns it as a dataframe. It does not take any inputs -- if you want to know what combinations of parameters are in your directory, this function will tell you.\n",
    "    \n",
    "```python\n",
    "params = analyzer.all_params()\n",
    "params.head()\n",
    "```\n",
    "```\n",
    ">\n",
    "    freq\tnMC\tntherm\titheta\tbeta\tlength\n",
    "0\t100.0\t50000.0\t5000.0\t0.785398\t1.6\t20.0\n",
    "1\t100.0\t50000.0\t5000.0\t0.000000\t1.6\t80.0\n",
    "2\t100.0\t50000.0\t5000.0\t0.000000\t1.6\t20.0\n",
    "3\t100.0\t50000.0\t5000.0\t1.570796\t1.6\t40.0\n",
    "4\t100.0\t50000.0\t5000.0\t2.356194\t1.6\t10.0\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4ddff3-fe45-47cb-9fae-9262550a93a6",
   "metadata": {},
   "source": [
    "If you want to collect data from more than one run, you can do this by only specifying which parameters you want in your dataframe, and the function will filter the data accordingly.\n",
    "\n",
    "```python\n",
    "param_dict = {\"length\": 10, \"itheta\":2.356194}\n",
    "filtered_data = analyzer.get_data(single_run = False, suppress_output = True, **param_dict)\n",
    "```\n",
    "\n",
    "If you want *all* the data, just leave out the parameter dictionary entirely and it won't filter anything.\n",
    "\n",
    "```python\n",
    "all_data = analyzer.get_data(single_run = False, suppress_output = True)\n",
    "```\n",
    "\n",
    "NOTE: I strongly recommend suppressing output if you are collecting more than 2 or 3 runs, as it will slow the program down and produce a flood of output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a191ab0-066a-4ff6-ada6-5c50a4f330c6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### get_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3f81d8-0ab7-48c7-949c-3121201b7119",
   "metadata": {},
   "source": [
    "```python\n",
    "get_data(self, single_run = False, corr = False, suppress_output = True, **kwargs)\n",
    "```\n",
    "\n",
    "This function will collect raw data from one or more runs and return it as a Pandas dataframe. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e5e140-c6eb-4efb-9240-ba2a1fdbbc2f",
   "metadata": {},
   "source": [
    "If you want to just get data from one simulation run (e.g. to check thermalization or autocorrelation), you should set ```single_run``` to ``` True```. If you want the correlation function data from that run, you should set ```corr``` to ```True``` -- otherwise it will return the observable data. If you want it to print out the parameter sets it's putting into the dataset, set ```suppress_output``` to ```False```\n",
    "\n",
    "You then need to specify what the parameters are for the run you want to see. You do this by creating a dictionary. When selecting a single one, you must ensure your dictionary has *all* the parameter values specified. The keys for these values are:\n",
    "* \"length\": length of the lattice in each direction\n",
    "* \"itheta\": value of the imaginary value used for theta -- actual number here, not an integer multiple of pi, but you can always use ```np.pi``` to specify it\n",
    "* \"beta\": for our purposes this will always be 1.6, but you need to specify it anyway\n",
    "* \"nMC\": number of steps in the Monte Carlo loop \n",
    "* \"ntherm\": number of steps in the thermalization loop\n",
    "* \"freq\": frequency with which the configurations were saved\n",
    "\n",
    "If you forget one of these, the function will remind you: \n",
    "\n",
    "```python\n",
    "param_dict = {\"length\": 10, \"freq\": 100, \"itheta\":2.356194, \"beta\":1.6, \"nMC\":50000}\n",
    "one_run = analyzer.get_data(single_run = True, suppress_output = False, **param_dict)\n",
    "\n",
    "```\n",
    "```\n",
    "Missing parameters in input: \n",
    "['ntherm']\n",
    "```\n",
    "So you know now to add in the \"ntherm\" you're looking for. Now it should work:\n",
    "```python\n",
    "param_dict = {\"length\": 10, \"freq\": 100, \"itheta\":2.356194, \"beta\":1.6, \"nMC\":50000, \"ntherm\": 5000}\n",
    "one_run = analyzer.get_data(single_run = True, suppress_output = False, **param_dict)\n",
    "```\n",
    "```\n",
    "freq 100\n",
    "nMC 50000\n",
    "ntherm 5000\n",
    "itheta 2.356194\n",
    "beta 1.6\n",
    "length 10\n",
    "```\n",
    "\n",
    "```python\n",
    "one_run.head(3)\n",
    "```\n",
    "```\n",
    "\tstep\t|phi|\tQ_L\tA_L\tS_L\tXi_L\tF_LRe\tF_LIm\tacc\tdt\t...\tQ_L_ta\tA_L_ta\tS_L_ta\tXi_L_ta\tcorr_length_Re\tcorr_length_Im\tF_Re_py\tF_Im_py\tmass_gap_Re\tmass_gap_Im\n",
    "0\t0\t100.0\t0.479179\t-183.166830\t-184.295868\t45.724903\t3.675149\t8.145972\t0.240000\t0.0\t...\t1\t3\t3\t4\t8.966957\t-3.640801\t0.916337\t0.890996\t0.095738\t0.038872\n",
    "1\t100\t100.0\t0.159302\t-195.846927\t-196.222275\t62.369185\t3.675149\t8.145972\t0.203762\t0.0\t...\t1\t3\t3\t4\t10.472580\t-4.252121\t0.916337\t0.890996\t0.081974\t0.033283\n",
    "2\t200\t100.0\t0.000000\t-199.012224\t-199.012224\t48.119359\t3.675149\t8.145972\t0.207164\t0.0\t...\t1\t3\t3\t4\t9.198745\t-3.734913\t0.916337\t0.890996\t0.093325\t0.037892\n",
    "3\t300\t100.0\t0.312655\t-179.261053\t-179.997729\t46.124447\t3.675149\t8.145972\t0.205681\t0.0\t...\t1\t3\t3\t4\t9.006048\t-3.656673\t0.916337\t0.890996\t0.095322\t0.038703\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552b15cc-750e-4c75-b2bb-e5044dde0d92",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### do_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4131ecae-84d2-4cb9-9785-fb2b8b6097c7",
   "metadata": {},
   "source": [
    "```python \n",
    "do_stats(self, therm = 0., stack = False, **kwargs)\n",
    "```\n",
    "This function collects data from the directory you specified when you initialized the object. If you want to filter the data by parameters, you just need to enter a parameter dictionary just like when using get_data() above, and it will filter to include only data that matches the parameter values you've entered.\n",
    "\n",
    "Once all the raw data is collected, it will perform some basic statistical analysis. All observables will have a mean and standard error calculated from data after thermalization. If you need to change the thermalization point, you can increase it by changing the ```therm``` argument in the function -- give it the fractional value of the data you want it to drop from the beginning of the dataset. So for example, if you have a run with ```nMC = 1000``` and ```freq = 10```, you will have a total of 100 steps in your dataset. If you set ```therm = 0.2```, it will drop the first 20 steps before computing the mean and standard error.\n",
    "\n",
    "After calculating means and standard errors, it also computes the autocorrelation time (the step at which the observable's autocorrelation value drops below 0.3), and then determines how long that run took to complete, saving that information in seconds, minutes, and hours.\n",
    "\n",
    "It saves all of this along with the parameters used in that run. The result is a very comprehensive dataframe. Here's an example of it run on a data directory with 25 runs in it:\n",
    "\n",
    "```python\n",
    "df_stats = analyzer.do_stats()\n",
    "df_stats.info()\n",
    "```\n",
    "```\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "RangeIndex: 25 entries, 0 to 24\n",
    "Data columns (total 41 columns):\n",
    " #   Column               Non-Null Count  Dtype  \n",
    "---  ------               --------------  -----  \n",
    " 0   length               25 non-null     float64\n",
    " 1   itheta               25 non-null     float64\n",
    " 2   beta                 25 non-null     float64\n",
    " 3   nMC                  25 non-null     float64\n",
    " 4   ntherm               25 non-null     float64\n",
    " 5   freq                 25 non-null     float64\n",
    " 6   |phi|_mean           25 non-null     float64\n",
    " 7   Q_L_mean             25 non-null     float64\n",
    " 8   A_L_mean             25 non-null     float64\n",
    " 9   S_L_mean             25 non-null     float64\n",
    " 10  Xi_L_mean            25 non-null     float64\n",
    " 11  F_LRe_mean           25 non-null     float64\n",
    " 12  F_LIm_mean           25 non-null     float64\n",
    " 13  acc_mean             25 non-null     float64\n",
    " 14  Q_L_ta               25 non-null     float64\n",
    " 15  A_L_ta               25 non-null     float64\n",
    " 16  S_L_ta               25 non-null     float64\n",
    " 17  Xi_L_ta              25 non-null     float64\n",
    " 18  corr_length_Re_mean  25 non-null     float64\n",
    " 19  corr_length_Im_mean  25 non-null     float64\n",
    " 20  F_Re_py_mean         25 non-null     float64\n",
    " 21  F_Im_py_mean         25 non-null     float64\n",
    " 22  mass_gap_Re_mean     25 non-null     float64\n",
    " 23  mass_gap_Im_mean     25 non-null     float64\n",
    " 24  |phi|_std            25 non-null     float64\n",
    " 25  Q_L_std              25 non-null     float64\n",
    " 26  A_L_std              25 non-null     float64\n",
    " 27  S_L_std              25 non-null     float64\n",
    " 28  Xi_L_std             25 non-null     float64\n",
    " 29  F_LRe_std            25 non-null     float64\n",
    " 30  F_LIm_std            25 non-null     float64\n",
    " 31  acc_std              25 non-null     float64\n",
    " 32  corr_length_Re_std   25 non-null     float64\n",
    " 33  corr_length_Im_std   25 non-null     float64\n",
    " 34  F_Re_py_std          25 non-null     float64\n",
    " 35  F_Im_py_std          25 non-null     float64\n",
    " 36  mass_gap_Re_std      25 non-null     float64\n",
    " 37  mass_gap_Im_std      25 non-null     float64\n",
    " 38  time (sec)           25 non-null     float64\n",
    " 39  time (min)           25 non-null     float64\n",
    " 40  time (hr)            25 non-null     float64\n",
    "dtypes: float64(41)\n",
    "memory usage: 8.1 KB\n",
    "\n",
    "```\n",
    "\n",
    "If I only want the analyzed data for runs where the lattice had a length of 20, I could modify this as follows:\n",
    "\n",
    "```python\n",
    "df_stats = analyzer.do_stats(**{\"length\":20})\n",
    "df_stats.info()\n",
    "```\n",
    "```\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "RangeIndex: 5 entries, 0 to 4\n",
    "Data columns (total 41 columns):\n",
    " #   Column               Non-Null Count  Dtype  \n",
    "---  ------               --------------  -----  \n",
    " 0   length               5 non-null      float64\n",
    " 1   itheta               5 non-null      float64\n",
    " 2   beta                 5 non-null      float64\n",
    " 3   nMC                  5 non-null      float64\n",
    " 4   ntherm               5 non-null      float64\n",
    " 5   freq                 5 non-null      float64\n",
    " 6   |phi|_mean           5 non-null      float64\n",
    " 7   Q_L_mean             5 non-null      float64\n",
    " 8   A_L_mean             5 non-null      float64\n",
    " 9   S_L_mean             5 non-null      float64\n",
    " 10  Xi_L_mean            5 non-null      float64\n",
    " 11  F_LRe_mean           5 non-null      float64\n",
    " 12  F_LIm_mean           5 non-null      float64\n",
    " 13  acc_mean             5 non-null      float64\n",
    " 14  Q_L_ta               5 non-null      float64\n",
    " 15  A_L_ta               5 non-null      float64\n",
    " 16  S_L_ta               5 non-null      float64\n",
    " 17  Xi_L_ta              5 non-null      float64\n",
    " 18  corr_length_Re_mean  5 non-null      float64\n",
    " 19  corr_length_Im_mean  5 non-null      float64\n",
    " 20  F_Re_py_mean         5 non-null      float64\n",
    " 21  F_Im_py_mean         5 non-null      float64\n",
    " 22  mass_gap_Re_mean     5 non-null      float64\n",
    " 23  mass_gap_Im_mean     5 non-null      float64\n",
    " 24  |phi|_std            5 non-null      float64\n",
    " 25  Q_L_std              5 non-null      float64\n",
    " 26  A_L_std              5 non-null      float64\n",
    " 27  S_L_std              5 non-null      float64\n",
    " 28  Xi_L_std             5 non-null      float64\n",
    " 29  F_LRe_std            5 non-null      float64\n",
    " 30  F_LIm_std            5 non-null      float64\n",
    " 31  acc_std              5 non-null      float64\n",
    " 32  corr_length_Re_std   5 non-null      float64\n",
    " 33  corr_length_Im_std   5 non-null      float64\n",
    " 34  F_Re_py_std          5 non-null      float64\n",
    " 35  F_Im_py_std          5 non-null      float64\n",
    " 36  mass_gap_Re_std      5 non-null      float64\n",
    " 37  mass_gap_Im_std      5 non-null      float64\n",
    " 38  time (sec)           5 non-null      float64\n",
    " 39  time (min)           5 non-null      float64\n",
    " 40  time (hr)            5 non-null      float64\n",
    "dtypes: float64(41)\n",
    "memory usage: 1.7 KB\n",
    "```\n",
    "Notice we now only have 5 entries in our dataframe, not 25. We can check that this worked:\n",
    "\n",
    "```python\n",
    "df_stats[\"length\"].unique()\n",
    "```\n",
    "```\n",
    "array([20.])\n",
    "```\n",
    "\n",
    "If you want the data returned using Pandas MultiIndex, set ```stack``` to ```True```, but MultiIndex doesn't always play well with seaborn and other plotting tools, so the default is ```False```."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdba58e-f301-4291-9d24-ba562df94cba",
   "metadata": {},
   "source": [
    "##### get_plot_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dec5711-804b-4ba7-b6c5-27930142022e",
   "metadata": {},
   "source": [
    "```python\n",
    "get_plot_data(self, obs = \"Q_L\", L = 10, beta = 1.6, nMC = 10000, ntherm = 1000, freq = 100, stack = False)\n",
    "```\n",
    "This function allows you to get the analyzed data for one observable as a function of itheta in order to plot it. You must specify all the other parameters\n",
    "\n",
    "This function could be modified in order to choose your independent variable, but right now all the plots we are interested in are functions of itheta, so it's unneccesary to plot the observable as a function of any other parameter. Seaborn can be used with the raw data to study systematic effects or other things that may be functions of ```nMC```, ```ntherm``` or ```L```. \n",
    "\n",
    "Just as with ```do_stats```, the default for stacking the dataframe is False, to make plotting with matplotlib simple. I\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c6fbf2-b77a-47aa-98e3-aa3b956056f6",
   "metadata": {},
   "source": [
    "##### get_corr_func"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adeb1d42-0793-40c4-901e-b72df700e3c0",
   "metadata": {},
   "source": [
    "```python\n",
    "get_corr_func(self,suppress_output = False,**kwargs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b7bcc8-a98c-475d-823b-afde9ca5d87b",
   "metadata": {},
   "source": [
    "#### Lattice Data Class Built-In Functions (\"Private\" or internal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8c7ca1-d46b-4f65-90bd-68242335e37d",
   "metadata": {},
   "source": [
    "##### calc_F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cada11-5f17-4825-bb44-0390e5b3d708",
   "metadata": {},
   "source": [
    "```python\n",
    "calc_F(self, **kwargs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2709ba64-ea15-494b-9c85-97b99dd631ef",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Analysis Notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca473327-59b3-49ed-ae35-3e9ad4ea8fe1",
   "metadata": {},
   "source": [
    "#### AnalysisTesting.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf8012a-42d1-408d-ad2d-41045f0e7173",
   "metadata": {},
   "source": [
    "#### PhiDist.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d232c1f2-fddb-4761-b38e-4a4cb763f029",
   "metadata": {},
   "source": [
    "#### DataComparison.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a413a8e9-3aff-413f-83ce-96e62a051b5f",
   "metadata": {},
   "source": [
    "#### SystematicsAndTiming.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d789686-0c65-4635-8b56-860d1f39c7d8",
   "metadata": {},
   "source": [
    "#### CorrelationFunction.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af50b05f-2023-43d3-90c3-548408e89fb6",
   "metadata": {},
   "source": [
    "#### Observables.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84726f69-add5-4d44-84d1-1ac4015a7efc",
   "metadata": {},
   "source": [
    "## Summary of Preliminary Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16565b7-c14e-4b27-a26b-cec50e66a38c",
   "metadata": {},
   "source": [
    "## Current issues and open questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04c9648-126b-458e-8d8c-9828b7a2f07e",
   "metadata": {},
   "source": [
    "Issues we are working on resolving:\n",
    "* The topological charge calculation ($Q_{L}$) is currently returning non-integer values\n",
    "* The correlation function at the smallest nonzero lattice momentum $2\\pi/L$ is a complex number, therefore so is our correlation length. Correlation length at imaginary $\\theta$ should be real "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2947a8-025f-491c-bf9c-769eddf1791f",
   "metadata": {},
   "source": [
    "Research questions and next steps\n",
    "* What can a ML model learn from the configuration data we have so far?\n",
    "* How will Complex Langevin results differ from these if at all?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1735349-3c49-49ed-a342-619cf50faf34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
